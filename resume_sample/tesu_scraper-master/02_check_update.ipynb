{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a scraper built to run on an EC2 instance to assist in TESU's open source materials accessibility initiative. It is the second of 3 notebooks used to scrape, check, create static sites, and update. As the name indicates, this notebook contains the checking and updating functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import neccessary packages. If any errors occur because of missing libraries, be sure to install them on \n",
    "# your EC2 instance.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from xml.etree.cElementTree import XML\n",
    "import zipfile\n",
    "from uuid import uuid4 as uuid\n",
    "from time import sleep\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv('run1.csv') # read in dataframe created in 01_scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 1: Does what we have in s3 == what we have in our .csv summary file?\n",
    "We retrieve a list of files from our s3 buckets html_content and pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from boto3.session import Session\n",
    "\n",
    "ACCESS_KEY='your_access_key'\n",
    "SECRET_KEY='your_secret_key'\n",
    "\n",
    "bucket_name = 'your_s3_bucket_name' # replace with your bucket name\n",
    "\n",
    "session = Session(aws_access_key_id=ACCESS_KEY,\n",
    "                  aws_secret_access_key=SECRET_KEY)\n",
    "s3 = session.resource('s3')\n",
    "your_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "html_list = []\n",
    "for s3_file in your_bucket.objects.all():\n",
    "    if str(s3_file.key)[0:13] == \"html_content/\" and len(str(s3_file.key)) > 5:\n",
    "        html_list.append(str(s3_file.key))\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "pdf_list = []\n",
    "for s3_file in your_bucket.objects.all():\n",
    "    if str(s3_file.key)[0:5] == \"pdfs/\" and len(str(s3_file.key)) > 5:\n",
    "        pdf_list.append(str(s3_file.key))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html_list = [x[13:] for x in html_list]\n",
    "pdf_list = [x[5:] for x in pdf_list]\n",
    "scraped_list = html_list + pdf_list\n",
    "scraped_list = sorted(scraped_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all files in ec2 csv\n",
    "no_youtube = data[data['comment'] != 'youtube video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#are they equal lengths?\n",
    "len(all_files) == len(scraped_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 2: So we know if they weren't captured by our scraper, they are not included on our CSV. Now to check each file for missing links\n",
    "Find all the documents where there are unequal link counts. This kind of error can happen if the page loads too slowly, or if the page blocks automated requests. We're ultimately going to find any discrepancies and see what can be solved by trying to request them again, and write work-arounds if we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we already loaded in what was scraped, stored as variable 'data'. We want to filter out anything that's a youtube video.\n",
    "data2 = data[data['comment'] != \"youtube video\"]\n",
    "data2['link_name'] = data2.apply(lambda x: x['idx'][0:8], axis =1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now to see if there's any \n",
    "file_list = os.listdir('../ec2docs')\n",
    "all_links = []\n",
    "unequal_records = [] #declares all_records as a list\n",
    "\n",
    "for file_count, file in enumerate(sorted(file_list)): #enumerate returns your list with each item numbered. \n",
    "                                              #so we can declare 2 variables to iterate over here, file_count and file \n",
    "    #open doc, from folder 'docs'\n",
    "    pathway = '../ec2docs/'+file\n",
    "    document = zipfile.ZipFile(pathway)\n",
    "    xml_content = document.read('word/document.xml')\n",
    "    document.close()\n",
    "    xml_str = str(xml_content)\n",
    "    \n",
    "    #create linklist for doc\n",
    "    link_list = re.findall('>http.*?\\<',xml_str) #it returns text starting with '>http', ending with '<', inclusive.\n",
    "            \n",
    "    link_list = [x[1:-1] for x in link_list] #shaves off first and last character of each item in the list. (it's a '<')\n",
    "    #replace &amp; with &, and other html entities.\n",
    "    link_list = [html.unescape(x) for x in link_list]\n",
    "    \n",
    "    if file_count == 0:\n",
    "        all_links = link_list\n",
    "    else:\n",
    "        all_links = all_links + link_list\n",
    "    for link_count, link in enumerate(link_list):\n",
    "        idx = \"link_%03d\" % (file_count+1) #this just creates the string. ie: \"doc_001\"\n",
    "    if len(link_list) != len(data[data['idx'].str.contains(idx)]):\n",
    "        unequal_records.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#see any unqual records?\n",
    "unequal_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare to just get links where there are mismatches, this can also be used for updating.\n",
    "all_links = list(data2['idx'])\n",
    "link_list = [x[0:12] for x in scraped_list]\n",
    "links_that_did_not_pull = sorted(list(set(all_links)-set(link_list)))\n",
    "to_scrape = data2[data2['idx'].isin(links_that_did_not_pull)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now to scrape just the links where we have a discrepancy:\n",
    "# this code also prints what the error is. In our case, it was mostly connection errors.\n",
    "from boto3.session import Session\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "ACCESS_KEY='your_access_key'\n",
    "SECRET_KEY='your_secret_key'\n",
    "\n",
    "bucket_name = 's3_bucket_name' # replace with your bucket name\n",
    "\n",
    "session = Session(aws_access_key_id=ACCESS_KEY,\n",
    "                  aws_secret_access_key=SECRET_KEY)\n",
    "s3 = session.resource('s3')\n",
    "your_bucket = s3.Bucket(bucket name)\n",
    "\n",
    "name_of_run = \"try2\"\n",
    "all_records = [] #declares all_records as a list\n",
    "# this will contain columns 'idx', 'docname', 'url' 'comment'\n",
    "\n",
    "tries = 3 # number of times retry a connection error\n",
    "\n",
    "for x in to_scrape.values.tolist():\n",
    "    idx = x[1]\n",
    "    link = x[3]\n",
    "    file = x[2]\n",
    "    comment = x[4]\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(link, headers={\"User-agent\": str(uuid())})\n",
    "        content = r.text\n",
    "\n",
    "        if \"youtube\" in link:\n",
    "            comment = \"youtube video\"\n",
    "        elif \".pdf\" in link:\n",
    "            content = r.content\n",
    "            key = \"pdfs/\" + idx + \".pdf\"\n",
    "            s3.Bucket(bucket_name).put_object(Key=key, Body=content)\n",
    "            comment = \"pdf file\"\n",
    "        else:\n",
    "            key = \"html_content/\" + idx + \".html\"\n",
    "            s3.Bucket(bucket_name).put_object(Key=key, Body=content)\n",
    "    except ConnectionError as e1:\n",
    "        print(idx, e1)\n",
    "        print('retrying', idx)\n",
    "        for t in range(tries):\n",
    "            sleep(10)\n",
    "            try:\n",
    "                r = requests.get(link, headers={\"User-agent\": str(uuid())})\n",
    "                content = r.text\n",
    "\n",
    "                if \"youtube\" in link:\n",
    "                    comment = \"youtube video\"\n",
    "                elif \".pdf\" in link:\n",
    "                    content = r.content\n",
    "                    key = \"pdfs/\" + idx + \".pdf\"\n",
    "                    s3.Bucket(bucket_name).put_object(Key=key, Body=content)\n",
    "                    comment = \"pdf file\"\n",
    "                else:\n",
    "                    key = \"html_content/\" + idx + \".html\"\n",
    "                    s3.Bucket(bucket_name).put_object(Key=key, Body=content)\n",
    "            except ConnectionError:\n",
    "                comment = \"connection error\"\n",
    "            except Exception:\n",
    "                comment = \"scraping error\"\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e2:\n",
    "        comment = \"scraping error\"\n",
    "        print(idx, e2)\n",
    "    finally:\n",
    "        #add to df \n",
    "        #appends a dictionary with keys \"idx\", \"docname\" and \"url\" to the list all_records.\n",
    "        all_records.append({'idx': idx, 'docname': file, 'url':link, 'comment':comment})\n",
    "        sleep(4)\n",
    "#make pandas df and store in runs folder\n",
    "df_long2 = pd.DataFrame(all_records, columns=['idx', 'docname', 'url', 'comment'])\n",
    "csv_name = name_of_run+'.csv'\n",
    "df_long2[['idx', 'docname','url','comment']].to_csv(csv_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
